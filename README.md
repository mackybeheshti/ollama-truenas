# Ollama TrueNAS Scale App

A production-ready TrueNAS Scale application for running Ollama with NVIDIA GPU support, optimized for Tesla P40 and multi-GPU setups.

[![TrueNAS](https://img.shields.io/badge/TrueNAS%20Scale-25.04+-blue.svg)](https://www.truenas.com/truenas-scale/)
[![Ollama](https://img.shields.io/badge/Ollama-Latest-green.svg)](https://ollama.ai)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

## üöÄ Features

- **Full NVIDIA GPU Support** - Optimized for Tesla P40 (24GB VRAM)
- **Real-time GPU Monitoring** - Track utilization, VRAM, temperature
- **GPU Sharing** - Compatible with Plex and Immich
- **Model Management UI** - Easy model download/deletion
- **Prometheus Metrics** - Export GPU and API metrics
- **API Security** - Optional authentication and CORS support
- **Persistent Storage** - Models survive updates
- **Web Dashboard** - Monitor and manage everything

## üìã Requirements

- TrueNAS Scale 25.04 (Dragonfish) or newer
- NVIDIA GPU with CUDA support
- NVIDIA Container Runtime configured
- At least 32GB storage for models
- 8GB RAM minimum (16GB recommended)

## üéØ Tesla P40 Specific Notes

The Tesla P40 is an excellent choice for AI inference with its 24GB VRAM, but note:
- **No NVENC** - P40 lacks video encoding capabilities
- **Passive Cooling** - Ensure adequate airflow
- **Compute Capability** - 6.1 (Pascal architecture)
- **Optimal for** - Large language models up to 70B parameters

## üì¶ Quick Installation

### Via TrueNAS Web UI

1. Navigate to **Apps** ‚Üí **Available Applications**
2. Search for "Ollama"
3. Click **Install**
4. Configure settings (see Configuration section)
5. Click **Save**

### Manual Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/ollama-truenas.git
cd ollama-truenas

# Run setup script
chmod +x setup.sh
./setup.sh
```

## ‚öôÔ∏è Configuration

### Basic Settings

| Setting | Default | Description |
|---------|---------|-------------|
| API Port | 11434 | Ollama API port |
| GPU Device | 0 | NVIDIA GPU index |
| Model Path | /mnt/models | Model storage location |
| VRAM Limit | 20GB | Max VRAM per model |
| Keep Alive | 5m | Model unload timeout |

### GPU Sharing Configuration

To share GPU with Plex and Immich, see [GPU_SHARING.md](docs/GPU_SHARING.md)

## üìä Monitoring

Access the monitoring dashboard at `http://your-nas-ip:3000`

Features:
- Real-time GPU metrics
- Model memory usage
- Request throughput
- Temperature monitoring
- Power consumption

## ü§ñ Model Management

### Via Web UI

Access at `http://your-nas-ip:8080`

### Via CLI

```bash
# List models
curl http://localhost:11434/api/tags

# Pull a model
curl -X POST http://localhost:11434/api/pull -d '{"name":"llama3.2:3b"}'

# Delete a model
curl -X DELETE http://localhost:11434/api/delete -d '{"name":"llama3.2:3b"}'
```

## üìö Documentation

- [GPU Sharing Guide](docs/GPU_SHARING.md) - Configure GPU sharing with other services
- [Model Management](docs/MODEL_MANAGEMENT.md) - Detailed model operations
- [Monitoring Setup](docs/MONITORING.md) - Configure Prometheus/Grafana
- [API Reference](docs/API.md) - Complete API documentation
- [Troubleshooting](docs/TROUBLESHOOTING.md) - Common issues and solutions

## üß™ Testing

Run the test suite:

```bash
cd tests
./run_tests.sh
```

## üêõ Troubleshooting

### GPU Not Detected

```bash
# Check NVIDIA runtime
nvidia-smi
docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi
```

### Models Not Persisting

Ensure the models directory has correct permissions:
```bash
chmod -R 755 /mnt/pool/ollama-models
chown -R apps:apps /mnt/pool/ollama-models
```

## üìà Performance Benchmarks (Tesla P40)

| Model | Tokens/sec | VRAM Usage | Concurrent Instances |
|-------|------------|------------|---------------------|
| Llama 3.2 3B | 45-50 | 3.5GB | 6 |
| Mistral 7B | 30-35 | 7.5GB | 3 |
| Llama 2 13B | 18-22 | 13GB | 1 |
| Mixtral 8x7B | 8-12 | 23GB | 1 |

## ü§ù Contributing

Contributions are welcome! Please read [CONTRIBUTING.md](CONTRIBUTING.md) for details.

## üìÑ License

This project is licensed under the MIT License - see [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- [Ollama](https://ollama.ai) team for the excellent LLM runtime
- [TrueNAS](https://www.truenas.com) team for the platform
- [NVIDIA](https://nvidia.com) for CUDA and container runtime

## üìû Support

- **Issues**: [GitHub Issues](https://github.com/yourusername/ollama-truenas/issues)
- **Discussions**: [GitHub Discussions](https://github.com/yourusername/ollama-truenas/discussions)
- **Wiki**: [Project Wiki](https://github.com/yourusername/ollama-truenas/wiki)
# test write by macky
